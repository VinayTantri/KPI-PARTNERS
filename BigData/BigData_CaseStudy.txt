CASE STUDY
Scenario 1:
Problem Statement: Danny has joined an IT company and they have going to conduct an assessment on BIGDATA which may have the questions below. He is not sure as to how to answer these.
You are an expert in Big Data Analytics and has come to for help. Help him with his queries.
1.Load the given textfile in HDFS.
[cloudera@quickstart ~]$ vi casestudy
[cloudera@quickstart ~]$ cat casestudy
It's a truly pleasant experience to read this book, actually I should confess that I laughed A LOT in the reading. The book is hilarious.
Besides the fun part, I was inspired by this book too. This book went through the early history of Personal Computer industry, gave the vivid silhouettes of the people, the companies and Silicon Valley in thisindustry. Mr.Cringely examined why today's Information Technology industry is what it is now, and how it became like this.
The book provided the facts and opinion about how the high tech companies succeeded, and how many more failed. Why Bill Gates is the richest person in the world, and how Steve Jobs and Steve Wozniak created the most beloved high tech company in the world.
It used to say that reading history can make people understand the rise and fall of things. We can learn the lessons from it, and get new ideas or patterns from the past success. Today Personal Computer is declining, and the focus is shifting to Smart Phone and Tablet. Although product is changing, the similar struggles, fights, winning and loss are still happening lively everyday in this industry, just like what it did in the old days.
[cloudera@quickstart ~]$ pwd
/home/cloudera
[cloudera@quickstart ~]$ hdfs dfs -put casestudy /home/cloudera
put: `/home/cloudera': No such file or directory
[cloudera@quickstart ~]$ hdfs dfs -put casestudy /user/cloudera
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera
Found 17 items
drwx------   - cloudera cloudera          0 2022-01-10 21:38 /user/cloudera/.staging
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:09 /user/cloudera/avro_json_write
-rw-r--r--   1 cloudera cloudera       1168 2022-01-24 09:16 /user/cloudera/casestudy
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:15 /user/cloudera/csv_dir
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 08:36 /user/cloudera/import_avro
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 /user/cloudera/json_avro_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:13 /user/cloudera/json_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:39 /user/cloudera/json_orc
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 /user/cloudera/json_orc_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:38 /user/cloudera/json_parquet
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 /user/cloudera/json_parquet_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:11 /user/cloudera/orc_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:14 /user/cloudera/parquet_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:11 /user/cloudera/parquet_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 13:40 /user/cloudera/part_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 14:01 /user/cloudera/part_dir2
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 09:04 /user/cloudera/zeyo_dir
[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/casestudy
It's a truly pleasant experience to read this book, actually I should confess that I laughed A LOT in the reading. The book is hilarious.
Besides the fun part, I was inspired by this book too. This book went through the early history of Personal Computer industry, gave the vivid silhouettes of the people, the companies and Silicon Valley in thisindustry. Mr.Cringely examined why today's Information Technology industry is what it is now, and how it became like this.
The book provided the facts and opinion about how the high tech companies succeeded, and how many more failed. Why Bill Gates is the richest person in the world, and how Steve Jobs and Steve Wozniak created the most beloved high tech company in the world.
It used to say that reading history can make people understand the rise and fall of things. We can learn the lessons from it, and get new ideas or patterns from the past success. Today Personal Computer is declining, and the focus is shifting to Smart Phone and Tablet. Although product is changing, the similar struggles, fights, winning and loss are still happening lively everyday in this industry, just like what it did in the old days.


2.Perform WordCount on the text file using mapreduce.
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera 
Found 19 items 
drwx------ - cloudera cloudera 0 2022-01-12 09:03 
/user/cloudera/.staging 
drwxr-xr-x - cloudera cloudera 0 2020-05-23 23:09 
/user/cloudera/avro_json_write 
drwxr-xr-x - cloudera cloudera 0 2020-05-22 22:15 
/user/cloudera/csv_dir 
drwxr-xr-x - cloudera cloudera 0 2022-01-12 09:03 
/user/cloudera/emp 
drwxr-xr-x - cloudera cloudera 0 2020-06-04 08:36 
/user/cloudera/import_avro 
drwxr-xr-x - cloudera cloudera 0 2020-05-23 22:56 
/user/cloudera/json_avro_1 
drwxr-xr-x - cloudera cloudera 0 2020-05-22 22:13 
/user/cloudera/json_dir 
drwxr-xr-x - cloudera cloudera 0 2020-05-23 22:39 
/user/cloudera/json_orc 
drwxr-xr-x - cloudera cloudera 0 2020-05-23 22:56 
/user/cloudera/json_orc_1 
drwxr-xr-x - cloudera cloudera 0 2020-05-23 22:38 
/user/cloudera/json_parquet 
drwxr-xr-x - cloudera cloudera 0 2020-05-23 22:56 
/user/cloudera/json_parquet_1 
drwxr-xr-x - cloudera cloudera 0 2020-05-22 22:11 
/user/cloudera/orc_dir 
drwxr-xr-x - cloudera cloudera 0 2020-05-22 22:14 
/user/cloudera/parquet_dir 
drwxr-xr-x - cloudera cloudera 0 2020-05-23 23:11 
/user/cloudera/parquet_json_write 
drwxr-xr-x - cloudera cloudera 0 2020-05-22 13:40 
drwxr-xr-x - cloudera cloudera 0 2020-05-22 13:40 
/user/cloudera/part_dir 
drwxr-xr-x - cloudera cloudera 0 2020-05-22 14:01 
/user/cloudera/part_dir2 
-rw-r--r-- 1 cloudera cloudera 81 2022-01-11 02:29 
/user/cloudera/table.csv 
-rw-r--r-- 1 cloudera cloudera 1173 2022-01-20 22:48 
/user/cloudera/words.txt 
drwxr-xr-x - cloudera cloudera 0 2020-06-04 09:04 
/user/cloudera/zeyo_dir 
[cloudera@quickstart ~]$ cat words.txt 
cat: words.txt: No such file or directory 
[cloudera@quickstart ~]$ cd user/cloudera 
bash: cd: user/cloudera: No such file or directory 
[cloudera@quickstart ~]$ cd user 
bash: cd: user: No such file or directory 
[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera 
cat: `/user/cloudera': Is a directory 
[cloudera@quickstart ~]$ hdfs dfs -cat words.txt /user/cloudera 
It's a truly pleasant experience to read this book, actually I should 
confess that I laughed A LOT in the reading. The book is hilarious. 
Besides the fun part, I was inspired by this book too. This book went through 
the early history of Personal Computer industry, gave the vivid silhouettes 
of the people, the companies and Silicon Valley in this industry. 
Mr.Cringely examined why today's Information Technology industry is what 
it is now, and how it became like this. 
The book provided the facts and opinion about how the high tech companies 
succeeded, and how many more failed. Why Bill Gates is the richest person 
in the world, and how Steve Jobs and Steve Wozniak created the most beloved 
high tech company in the world. 
It used to say that reading history can make people understand the rise 
and fall of things. We can learn the lessons from it, and get new ideas 
or patterns from the past success. Today Personal Computer is declining, 
and the focus is shifting to Smart Phone and Tablet. Although product is 
changing, the similar struggles, fights, winning and loss are still 
happening lively everyday in this industry, just like what it did in the 
old days. 
cat: `/user/cloudera': Is a directory 
[cloudera@quickstart ~]$ ^C 
[cloudera@quickstart ~]$ hadoop jar 
/usr/lib/hadoop-mapreduce/hadoop-map-reduce-examples.jar 
Not a valid JAR: 
/usr/lib/hadoop-mapreduce/hadoop-map-reduce-examples.jar 
[cloudera@quickstart ~]$ hadoop jar 
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount 
/user/cloudera/sample_kpi.txt /user/cloudera/output 
22/01/20 23:12:10 INFO client.RMProxy: Connecting to ResourceManager at
quickstart.cloudera/127.0.0.1:8032 
22/01/20 23:12:13 INFO mapreduce.JobSubmitter: Cleaning up the staging 
area /user/cloudera/.staging/job_1642747289338_0001 
22/01/20 23:12:13 WARN security.UserGroupInformation: 
PriviledgedActionException as:cloudera (auth:SIMPLE) 
cause:org.apache.hadoop.mapreduce.lib.input.InvalidInputException: 
Input path does not exist: 
hdfs://quickstart.cloudera:8020/user/cloudera/sample_kpi.txt 
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path 
does not exist: 
hdfs://quickstart.cloudera:8020/user/cloudera/sample_kpi.txt 
 at 
org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedLi
stStatus(FileInputFormat.java:323) 
 at 
org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileI
nputFormat.java:265) 
 at 
org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileIn
putFormat.java:387) 
 at 
org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.j
ava:305) 
 at 
org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java
:322) 
 at 
org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitte
r.java:200) 
 at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307) 
 at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304) 
 at java.security.AccessController.doPrivileged(Native Method) 
 at javax.security.auth.Subject.doAs(Subject.java:422) 
 at 
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformat
ion.java:1917) 
 at org.apache.hadoop.mapreduce.Job.submit(Job.java:1304) 
 at 
org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1325) 
 at org.apache.hadoop.examples.WordCount.main(WordCount.java:87) 
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
 at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.j
ava:62) 
 at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccess
orImpl.java:43) 
 at java.lang.reflect.Method.invoke(Method.java:498) 
 at 
org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(Program
Driver.java:71) 
 at 
org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144) 
 at 
org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74) 
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
 at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.j
ava:62) 
 at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccess
orImpl.java:43) 
 at java.lang.reflect.Method.invoke(Method.java:498) 
 at org.apache.hadoop.util.RunJar.run(RunJar.java:221) 
 at org.apache.hadoop.util.RunJar.main(RunJar.java:136) 
[cloudera@quickstart ~]$ hadoop jar 
/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount 
/user/cloudera/words.txt /user/cloudera/output 
22/01/20 23:12:45 INFO client.RMProxy: Connecting to ResourceManager at 
quickstart.cloudera/127.0.0.1:8032 
22/01/20 23:12:49 INFO input.FileInputFormat: Total input paths to process 
: 1 
22/01/20 23:12:50 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException 
 at java.lang.Object.wait(Native Method) 
 at java.lang.Thread.join(Thread.java:1252) 
 at java.lang.Thread.join(Thread.java:1326) 
 at 
org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFS
OutputStream.java:967) 
 at 
org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutput
Stream.java:705) 
 at 
org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStrea
m.java:894) 
22/01/20 23:12:50 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException 
 at java.lang.Object.wait(Native Method) 
 at java.lang.Thread.join(Thread.java:1252) 
 at java.lang.Thread.join(Thread.java:1326) 
 at 
org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFS
OutputStream.java:967) 
 at 
org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutput
Stream.java:705) 
 at 
org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStrea
m.java:894) 
22/01/20 23:12:50 INFO mapreduce.JobSubmitter: number of splits:1 
22/01/20 23:12:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: 
job_1642747289338_0002 
22/01/20 23:12:52 INFO impl.YarnClientImpl: Submitted application 
application_1642747289338_0002 
22/01/20 23:12:53 INFO mapreduce.Job: The url to track the job: 
http://quickstart.cloudera:8088/proxy/application_1642747289338_0002/ 
22/01/20 23:12:53 INFO mapreduce.Job: Running job: job_1642747289338_0002 
22/01/20 23:13:33 INFO mapreduce.Job: Job job_1642747289338_0002 running 
in uber mode : false 
22/01/20 23:13:33 INFO mapreduce.Job: map 0% reduce 0% 
22/01/20 23:14:05 INFO mapreduce.Job: map 100% reduce 0% 
22/01/20 23:14:35 INFO mapreduce.Job: map 100% reduce 100% 
22/01/20 23:14:37 INFO mapreduce.Job: Job job_1642747289338_0002 
completed successfully 
22/01/20 23:14:38 INFO mapreduce.Job: Counters: 49 
 File System Counters 
 FILE: Number of bytes read=1261 
 FILE: Number of bytes written=297411 
 FILE: Number of read operations=0 
 FILE: Number of large read operations=0 
 FILE: Number of write operations=0 
 HDFS: Number of bytes read=1293 
 HDFS: Number of bytes written=1143 
 HDFS: Number of read operations=6 
 HDFS: Number of large read operations=0 
 HDFS: Number of write operations=2 
 Job Counters 
 Launched map tasks=1 
 Launched reduce tasks=1 
 Data-local map tasks=1 
 Total time spent by all maps in occupied slots (ms)=14978560 
 Total time spent by all reduces in occupied slots (ms)=14747648 
 Total time spent by all map tasks (ms)=29255 
 Total time spent by all reduce tasks (ms)=28804 
 Total vcore-milliseconds taken by all map tasks=29255 
 Total vcore-milliseconds taken by all reduce tasks=28804 
 Total megabyte-milliseconds taken by all map tasks=14978560 
 Total megabyte-milliseconds taken by all reduce tasks=14747648 
 Map-Reduce Framework 
 Map input records=9 
 Map output records=203 
 Map output bytes=1980 
 Map output materialized bytes=1257 
 Input split bytes=120 
 Combine input records=203 
 Combine output records=134 
 Reduce input groups=134 
 Reduce shuffle bytes=1257 
 Reduce input records=134 
 Reduce output records=134 
 Spilled Records=268 
 Shuffled Maps =1 
 Failed Shuffles=0 
 Merged Map outputs=1 
 GC time elapsed (ms)=1352 
 CPU time spent (ms)=2520 
 Physical memory (bytes) snapshot=251547648 
 Virtual memory (bytes) snapshot=3890036736 
 Total committed heap usage (bytes)=101449728 
 Shuffle Errors 
 BAD_ID=0 
 CONNECTION=0 
 IO_ERROR=0 
 WRONG_LENGTH=0 
 WRONG_MAP=0 
 WRONG_REDUCE=0 
 File Input Format Counters 
 Bytes Read=1173 
 File Output Format Counters 
 Bytes Written=1143 
[cloudera@quickstart ~]$ 
[cloudera@quickstart ~]$ hadoop dfs -ls /user/cloudera/output 
DEPRECATED: Use of this script to execute hdfs command is deprecated. 
Instead use the hdfs command for it. 
Found 2 items 
-rw-r--r-- 1 cloudera cloudera 0 2022-01-20 23:14 
/user/cloudera/output/_SUCCESS 
-rw-r--r-- 1 cloudera cloudera 1143 2022-01-20 23:14 
/user/cloudera/output/part-r-00000 
[cloudera@quickstart ~]$ hadoop dfs -cat 
/user/cloudera/output/part-r-00000 
DEPRECATED: Use of this script to execute hdfs command is deprecated. 
Instead use the hdfs command for it. 
A 1 
Although 1 
Besides 1 
Bill 1 
Computer 2 
Gates 1 
I 3 
Information 1 
It 1 
It's 1 
Jobs 1 
LOT 1 
Mr.Cringely 1 
Personal 2 
Phone 1 
Silicon 1 
Smart 1 
Steve 2 
Tablet. 1 
Technology 1 
The 2 
This 1 
Today 1 
Valley 1 
We 1 
Why 1 
Wozniak 1 
a 1 
about 1 
actually 1 
and 11 
are 1 
became 1 
beloved 1 
book 4 
book, 1 
by 1 
can 2 
changing, 1 
companies 2 
company 1 
confess 1 
created 1 
days. 1 
declining, 1 
did 1 
early 1 
everyday 1 
examined 1 
experience 1 
facts 1 
failed. 1 
fall 1 
fights, 1 
focus 1 
from 2 
fun 1 
gave 1 
get 1 
happening 1 
high 2 
hilarious. 1 
history 2 
how 4 
ideas 1 
in 6 
industry 1 
industry, 2 
industry. 1 
inspired 1 
is 7 
it 3 
it, 1 
just 1 
laughed 1 
learn 1 
lessons 1 
like 2 
lively 1 
loss 1 
make 1 
many 1 
more 1 
most 1 
new 1 
now, 1 
of 3 
old 1 
opinion 1 
or 1 
part, 1 
past 1 
patterns 1 
people 1 
people, 1 
person 1 
pleasant 1 
product 1 
provided 1 
read 1 
reading 1 
reading. 1 
richest 1 
rise 1 
say 1 
shifting 1 
should 1 
silhouettes 1 
similar 1 
still 1 
struggles, 1 
succeeded, 1 
success. 1 
tech 2 
that 2 
the 18 
things. 1 
this 4 
this. 1 
through 1 
to 3 
today's 1 
too. 1 
truly 1 
understand 1 
used 1 
vivid 1 
was 1 
went 1 
what 2 
why 1 
winning 1 
world, 1 
world. 1 


3.Create a HBase table ‘Census’ using java with Column Family as ‘Personal’, ‘Professional’.
[cloudera@quickstart ~]$ hbase shell
OpenJDK 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release
22/01/24 09:28:30 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
HBase Shell; enter 'help<RETURN>' for list of supported commands.
Type "exit<RETURN>" to leave the HBase Shell
Version 1.2.0-cdh5.13.0, rUnknown, Wed Oct  4 11:16:18 PDT 2017
hbase(main):004:0> describe 'census'
Table census is ENABLED                                                                                                                                                                                          
census                                                                                                                                                                                                           
COLUMN FAMILIES DESCRIPTION                                                                                                                                                                                      
{NAME => 'personal', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLO
CKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}                                                                                                                                               
{NAME => 'professional', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0',
 BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}                                                                                                                                           
2 row(s) in 0.2400 seconds


4.Put 2 rows in the Census table each having columns name and gender in personal and occupation in professional  and display data using HBase shell.
[cloudera@quickstart ~]$ hbase shell
OpenJDK 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release
22/01/24 09:40:22 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
HBase Shell; enter 'help<RETURN>' for list of supported commands.
Type "exit<RETURN>" to leave the HBase Shell
Version 1.2.0-cdh5.13.0, rUnknown, Wed Oct  4 11:16:18 PDT 2017

hbase(main):001:0> put 'census',1,'personal:name','Vinay'
0 row(s) in 0.6900 seconds

hbase(main):002:0> put 'census',1,'personal:gender','Male'
0 row(s) in 0.0170 seconds

hbase(main):003:0> put 'census',2,'professional:occupation','Engineer'
0 row(s) in 0.0490 seconds

hbase(main):004:0> put 'census',1,'personal:name','Dhanush'
0 row(s) in 0.0320 seconds

hbase(main):005:0> put 'census',1,'personal:gender','Male'
0 row(s) in 0.0680 seconds

hbase(main):006:0> put 'census',2,'professional:occupation','Doctor'
0 row(s) in 0.0140 seconds

hbase(main):007:0> scan 'census'
ROW                                                   COLUMN+CELL                                                                                                                                                
 1                                                    column=personal:gender, timestamp=1643046214793, value=Male                                                                                                
 1                                                    column=personal:name, timestamp=1643046205154, value=Dhanush                                                                                               
 2                                                    column=professional:occupation, timestamp=1643046223557, value=Doctor                                                                                      
2 row(s) in 0.1350 seconds


5.Load the groceries data file using PigStorage in Grunt shell with a schema and describe and display the data.

